{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from happytransformer import HappyTextClassification\n",
    "from pathlib import Path\n",
    "from newsapi import NewsApiClient\n",
    "from sentiment_analisys import get_digests_by_terms_async, classify_digests_async, Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/08/2022 20:39:33 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path = [str(Path().resolve().parent.joinpath(\"nltk_data\"))]\n",
    "\n",
    "# nltk.download(\"punkt\", download_dir=\"./nltk_data\")\n",
    "\n",
    "# Init pass 9gPj8KZ8XJsLbcT\n",
    "api = NewsApiClient(api_key='98d763c2280d4820913c4abf3ff0270b')\n",
    "\n",
    "# classifier = HappyTextClassification(\n",
    "#     model_type=\"DISTILBERT\", num_labels=2,\n",
    "#     model_name=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "classifier = HappyTextClassification(\n",
    "    model_type=\"BERT\", num_labels=3,\n",
    "    model_name=\"ProsusAI/finbert\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [source[\"id\"] for source in api.get_sources()[\"sources\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordListCorpusReader' object has no attribute '_LazyCorpusLoader__args'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\chunk\\named_entity.py:48\u001B[0m, in \u001B[0;36mNEChunkParserTagger._english_wordlist\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 48\u001B[0m     wl \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_en_wordlist\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NEChunkParserTagger' object has no attribute '_en_wordlist'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# sources = None\u001B[39;00m\n\u001B[0;32m      6\u001B[0m terms \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbitcoin,ethereum\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 8\u001B[0m digests_by_term \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m get_digests_by_terms_async(terms, sources, api)\n\u001B[0;32m     10\u001B[0m predictions: [Prediction] \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     11\u001B[0m     Prediction(\n\u001B[0;32m     12\u001B[0m         results\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mawait\u001B[39;00m classify_digests_async(digests, classifier),\n\u001B[0;32m     13\u001B[0m         term\u001B[38;5;241m=\u001B[39mterm\n\u001B[0;32m     14\u001B[0m     ) \u001B[38;5;28;01mfor\u001B[39;00m term, digests \u001B[38;5;129;01min\u001B[39;00m digests_by_term\n\u001B[0;32m     15\u001B[0m ]\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\sentiment_analisys.py:80\u001B[0m, in \u001B[0;36mget_digests_by_terms_async\u001B[1;34m(terms, sources, client)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_digests_by_terms_async\u001B[39m(terms: \u001B[38;5;28mstr\u001B[39m, sources: \u001B[38;5;28mstr\u001B[39m, client: NewsApiClient):\n\u001B[1;32m---> 80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\n\u001B[0;32m     81\u001B[0m         \u001B[38;5;241m*\u001B[39m[get_digests_async(term, sources, client) \u001B[38;5;28;01mfor\u001B[39;00m term \u001B[38;5;129;01min\u001B[39;00m terms\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m     82\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\sentiment_analisys.py:72\u001B[0m, in \u001B[0;36mget_digests_async\u001B[1;34m(term, sources, client, from_param, language)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_digests_async\u001B[39m(term: \u001B[38;5;28mstr\u001B[39m, sources: \u001B[38;5;28mstr\u001B[39m, client: NewsApiClient,\n\u001B[0;32m     63\u001B[0m                             from_param\u001B[38;5;241m=\u001B[39mdate\u001B[38;5;241m.\u001B[39mtoday() \u001B[38;5;241m-\u001B[39m timedelta(days\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m),\n\u001B[0;32m     64\u001B[0m                             language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     66\u001B[0m     response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_everything(term,\n\u001B[0;32m     67\u001B[0m                                      sources\u001B[38;5;241m=\u001B[39msources,\n\u001B[0;32m     68\u001B[0m                                      \u001B[38;5;66;03m# category='business',\u001B[39;00m\n\u001B[0;32m     69\u001B[0m                                      from_param\u001B[38;5;241m=\u001B[39mfrom_param,\n\u001B[0;32m     70\u001B[0m                                      language\u001B[38;5;241m=\u001B[39mlanguage)\n\u001B[1;32m---> 72\u001B[0m     digests \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\n\u001B[0;32m     73\u001B[0m         \u001B[38;5;241m*\u001B[39m[process_article_async(Article(article[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m\"\u001B[39m])) \u001B[38;5;28;01mfor\u001B[39;00m article \u001B[38;5;129;01min\u001B[39;00m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marticles\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m     74\u001B[0m     )\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m term, digests\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\sentiment_analisys.py:59\u001B[0m, in \u001B[0;36mprocess_article_async\u001B[1;34m(article)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_article_async\u001B[39m(article: Article):\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mto_thread(process_article, article)\n",
      "File \u001B[1;32mD:\\Dev\\env\\python-3.10.4-amd64\\lib\\asyncio\\threads.py:25\u001B[0m, in \u001B[0;36mto_thread\u001B[1;34m(func, *args, **kwargs)\u001B[0m\n\u001B[0;32m     23\u001B[0m ctx \u001B[38;5;241m=\u001B[39m contextvars\u001B[38;5;241m.\u001B[39mcopy_context()\n\u001B[0;32m     24\u001B[0m func_call \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mpartial(ctx\u001B[38;5;241m.\u001B[39mrun, func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mrun_in_executor(\u001B[38;5;28;01mNone\u001B[39;00m, func_call)\n",
      "File \u001B[1;32mD:\\Dev\\env\\python-3.10.4-amd64\\lib\\concurrent\\futures\\thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\sentiment_analisys.py:42\u001B[0m, in \u001B[0;36mprocess_article\u001B[1;34m(article)\u001B[0m\n\u001B[0;32m     40\u001B[0m article\u001B[38;5;241m.\u001B[39mnlp()\n\u001B[0;32m     41\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(article\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39msplit()[:\u001B[38;5;241m300\u001B[39m])\n\u001B[1;32m---> 42\u001B[0m ners \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[43mget_ners\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     43\u001B[0m keywords \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(article\u001B[38;5;241m.\u001B[39mkeywords)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Digest(\n\u001B[0;32m     45\u001B[0m     url\u001B[38;5;241m=\u001B[39marticle\u001B[38;5;241m.\u001B[39murl,\n\u001B[0;32m     46\u001B[0m     html\u001B[38;5;241m=\u001B[39marticle\u001B[38;5;241m.\u001B[39mhtml,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     54\u001B[0m     ners\u001B[38;5;241m=\u001B[39mners\n\u001B[0;32m     55\u001B[0m )\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\sentiment_analisys.py:95\u001B[0m, in \u001B[0;36mget_ners\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_ners\u001B[39m(text):\n\u001B[1;32m---> 95\u001B[0m     chunked \u001B[38;5;241m=\u001B[39m \u001B[43mne_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpos_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     continuous_chunk \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     97\u001B[0m     current_chunk \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\chunk\\__init__.py:184\u001B[0m, in \u001B[0;36mne_chunk\u001B[1;34m(tagged_tokens, binary)\u001B[0m\n\u001B[0;32m    182\u001B[0m     chunker_pickle \u001B[38;5;241m=\u001B[39m _MULTICLASS_NE_CHUNKER\n\u001B[0;32m    183\u001B[0m chunker \u001B[38;5;241m=\u001B[39m load(chunker_pickle)\n\u001B[1;32m--> 184\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mchunker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtagged_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\chunk\\named_entity.py:127\u001B[0m, in \u001B[0;36mNEChunkParser.parse\u001B[1;34m(self, tokens)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens):\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;124;03m    Each token should be a pos-tagged word\u001B[39;00m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 127\u001B[0m     tagged \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tagger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m     tree \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tagged_to_parse(tagged)\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tree\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\tag\\sequential.py:61\u001B[0m, in \u001B[0;36mSequentialBackoffTagger.tag\u001B[1;34m(self, tokens)\u001B[0m\n\u001B[0;32m     59\u001B[0m tags \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(tokens)):\n\u001B[1;32m---> 61\u001B[0m     tags\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtag_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(tokens, tags))\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\tag\\sequential.py:81\u001B[0m, in \u001B[0;36mSequentialBackoffTagger.tag_one\u001B[1;34m(self, tokens, index, history)\u001B[0m\n\u001B[0;32m     79\u001B[0m tag \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tagger \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_taggers:\n\u001B[1;32m---> 81\u001B[0m     tag \u001B[38;5;241m=\u001B[39m \u001B[43mtagger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tag \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\tag\\sequential.py:647\u001B[0m, in \u001B[0;36mClassifierBasedTagger.choose_tag\u001B[1;34m(self, tokens, index, history)\u001B[0m\n\u001B[0;32m    645\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchoose_tag\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens, index, history):\n\u001B[0;32m    646\u001B[0m     \u001B[38;5;66;03m# Use our feature detector to get the featureset.\u001B[39;00m\n\u001B[1;32m--> 647\u001B[0m     featureset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_detector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    649\u001B[0m     \u001B[38;5;66;03m# Use the classifier to pick a tag.  If a cutoff probability\u001B[39;00m\n\u001B[0;32m    650\u001B[0m     \u001B[38;5;66;03m# was specified, then check that the tag's probability is\u001B[39;00m\n\u001B[0;32m    651\u001B[0m     \u001B[38;5;66;03m# higher than that cutoff first; otherwise, return None.\u001B[39;00m\n\u001B[0;32m    652\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cutoff_prob \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\tag\\sequential.py:694\u001B[0m, in \u001B[0;36mClassifierBasedTagger.feature_detector\u001B[1;34m(self, tokens, index, history)\u001B[0m\n\u001B[0;32m    684\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeature_detector\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens, index, history):\n\u001B[0;32m    685\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    686\u001B[0m \u001B[38;5;124;03m    Return the feature detector that this tagger uses to generate\u001B[39;00m\n\u001B[0;32m    687\u001B[0m \u001B[38;5;124;03m    featuresets for its classifier.  The feature detector is a\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    692\u001B[0m \u001B[38;5;124;03m    See ``classifier()``\u001B[39;00m\n\u001B[0;32m    693\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 694\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_feature_detector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\chunk\\named_entity.py:101\u001B[0m, in \u001B[0;36mNEChunkParserTagger._feature_detector\u001B[1;34m(self, tokens, index, history)\u001B[0m\n\u001B[0;32m     90\u001B[0m     nextnextpos \u001B[38;5;241m=\u001B[39m tokens[index \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m     92\u001B[0m \u001B[38;5;66;03m# 89.6\u001B[39;00m\n\u001B[0;32m     93\u001B[0m features \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m: shape(word),\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwordlen\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(word),\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprefix3\u001B[39m\u001B[38;5;124m\"\u001B[39m: word[:\u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuffix3\u001B[39m\u001B[38;5;124m\"\u001B[39m: word[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m:]\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos\u001B[39m\u001B[38;5;124m\"\u001B[39m: pos,\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m\"\u001B[39m: word,\n\u001B[1;32m--> 101\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men-wordlist\u001B[39m\u001B[38;5;124m\"\u001B[39m: (word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_english_wordlist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m),\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprevtag\u001B[39m\u001B[38;5;124m\"\u001B[39m: prevtag,\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprevpos\u001B[39m\u001B[38;5;124m\"\u001B[39m: prevpos,\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnextpos\u001B[39m\u001B[38;5;124m\"\u001B[39m: nextpos,\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprevword\u001B[39m\u001B[38;5;124m\"\u001B[39m: prevword,\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnextword\u001B[39m\u001B[38;5;124m\"\u001B[39m: nextword,\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword+nextpos\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mword\u001B[38;5;241m.\u001B[39mlower()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m+\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnextpos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpos+prevtag\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m+\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprevtag\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape+prevtag\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprevshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m+\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprevtag\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    110\u001B[0m }\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\chunk\\named_entity.py:52\u001B[0m, in \u001B[0;36mNEChunkParserTagger._english_wordlist\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m words\n\u001B[1;32m---> 52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_en_wordlist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[43mwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men-basic\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     53\u001B[0m     wl \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_en_wordlist\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wl\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLazyCorpusLoader object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "File \u001B[1;32mD:\\Dev\\source\\PLN-Notebook\\venv-cp310\\lib\\site-packages\\nltk\\corpus\\util.py:95\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     89\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls(root, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;66;03m# match that of the corpus.\u001B[39;00m\n\u001B[1;32m---> 95\u001B[0m args, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__args\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs\n\u001B[0;32m     96\u001B[0m name, reader_cls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m \u001B[38;5;241m=\u001B[39m corpus\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'WordListCorpusReader' object has no attribute '_LazyCorpusLoader__args'"
     ]
    }
   ],
   "source": [
    "\n",
    "# sources = \"business-insider,crypto-coins-news,financial-post,fortune,google-news,info-money,hacker-news,reddit-r-all,Bloomberg\"\n",
    "sources = \"business-insider,crypto-coins-news,financial-post,fortune,google-news,info-money,hacker-news,reddit-r-all,buzzfeed,bbc-news,cbc-news,cnn,engadget,infobae,mashable\"\n",
    "\n",
    "# sources = None\n",
    "\n",
    "terms = \"bitcoin,ethereum\"\n",
    "\n",
    "digests_by_term = await get_digests_by_terms_async(terms, sources, api)\n",
    "\n",
    "predictions: [Prediction] = [\n",
    "    Prediction(\n",
    "        results=await classify_digests_async(digests, classifier),\n",
    "        term=term\n",
    "    ) for term, digests in digests_by_term\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [digest.ners for (digest, classification) in predictions[0].results]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [digest.keywords for (digest, classification) in predictions[0].results]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = [(prediction.term, classification.label, classification.score, digest.url)\n",
    "          for prediction in predictions\n",
    "          for (digest, classification) in prediction.results]\n",
    "\n",
    "df = pd.DataFrame(values, columns= ['term','label', 'score', 'source'])\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for prediction in predictions:\n",
    "    term = prediction.term\n",
    "    labels = {\"negative\": 0, \"positive\": 0, \"neutral\": 0}\n",
    "    results_count = len(prediction.results)\n",
    "    for digest, classification in prediction.results:\n",
    "        labels[classification.label] += 1\n",
    "        total = labels[\"positive\"] + labels[\"negative\"] + labels[\"neutral\"]\n",
    "        # print(f\"{term} = {classification.label}\")\n",
    "        if results_count == total:\n",
    "            positive_ratio = labels[\"positive\"] / total\n",
    "            negative_ratio = labels[\"negative\"] / total\n",
    "            neutral_ratio = labels[\"neutral\"] / total\n",
    "            should_buy = positive_ratio > negative_ratio and positive_ratio > 0\n",
    "            print(f\"{term}: {'should buy' if should_buy else 'should not buy'}\")\n",
    "            print(f\"positive = {positive_ratio}\")\n",
    "            print(f\"negative = {negative_ratio}\")\n",
    "            print(f\"neutral = {neutral_ratio}\")\n",
    "            print(\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}